<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<!-- saved from url=(0061)http://staff.ustc.edu.cn/~xjchen99/FoldingCartons.html -->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><HTML 
xmlns="http://www.w3.org/1999/xhtml"><HEAD><META content="IE=11.0000" 
http-equiv="X-UA-Compatible">
 
<META http-equiv="Content-Type" content="text/html; charset=utf-8"> 
<TITLE>Dual Progressive Prototype Network for Generalized Zero-Shot Learning</TITLE> 
<STYLE type="text/css">
.ys01 {
	font-size: 30px;
	font-family: Arial, Helvetica, sans-serif;
	font-weight: bold;
	color: #57524C;
}
.ys02 {
	font-family: Arial, Helvetica, sans-serif;
	font-weight: bold;
	color: #6293A2;
	text-align: center;
}
.ys2 {
}
.ys2 {
	font-weight: bold;
	font-family: Arial, Helvetica, sans-serif;
	font-size: 16px;
	color: #6293A2;
	text-align: center;
}
.ys2 sup {
	color: #57524C;
}
.ys3 {
	font-weight: bold;
}
.ys3 {
	font-family: Arial, Helvetica, sans-serif;
}
.ys3 {
	text-align: center;
	color: #57524C;
}
.ys3 {
}
.ys4 {
	text-align: center;
}
.ys2 .ys2 {
	color: #57524C;
}
.ys5 {
	font-weight: bold;
	font-family: Arial, Helvetica, sans-serif;
}
.ys6 {
	font-family: Arial, Helvetica, sans-serif;
}
.ys7 {
	text-align: center;
}
a:link {
	text-decoration: none;
}
a:visited {
	text-decoration: none;
}
a:hover {
	text-decoration: underline;
}
a:active {
	text-decoration: none;
}
.teaser {
	font-weight: bold;
	font-family: "Times New Roman", Times, serif;
	color: #57524C;
	text-align: left;
}
.text {
	font-family: "Times New Roman", Times, serif;
	color: #57524C;
	text-align: justify;
	font-weight: normal;
}
.title1 {
	font-family: Arial, Helvetica, sans-serif;
	color: #6293A2;
	font-weight: bold;
	text-align: left;
	font-size: 18px;
}
.text2 {
	text-align: justify;
	font-family: "Times New Roman", Times, serif;
	color: #57524C;
	font-weight: normal;
}
.title1 .teaser {
	text-align: justify;
}
.copyright {
	font-family: "Times New Roman", Times, serif;
	color: #57524C;
	text-align: right;
}
.copy {
	font-family: "Times New Roman", Times, serif;
	text-align: right;
	color: #57524C;
}
.download {
}
.download {
	font-family: Arial, Helvetica, sans-serif;
}
.download {
	font-weight: bold;
}
.download {
	color: #6293A2;
}
body {
	background-color: #FFFFFF;
	text-align: center;
}
.teaser {
	text-align: justify;
}
.copy1 {
	text-align: right;
	color: #000;
	font-family: Arial, Helvetica, sans-serif;
	font-size: 14px;
}
.STYLE17 {font-size: medium}
.style1 {	FONT-FAMILY: "Times New Roman", Times, Verdana, Arial, Helvetica, sans-serif ;
	FONT-SIZE: 18px
}
</STYLE>
 
<META name="GENERATOR" content="MSHTML 11.00.9600.18538"></HEAD> 
<BODY>
<TABLE width="1044" align="center" background="">
  <TBODY>
  <TR>
    <TD width="1044" height="3355" align="center" valign="top">
      <P>&nbsp;</P>
      <TABLE width="900">
        <TBODY>
        <TR>
          <TD>
            <P align="center" class="ys01">Dual Progressive Prototype Network for Generalized Zero-Shot Learning</P>
            <P align="center" class="ys2">Chaoqun Wang, Shaobo Min, Xuejin Chen*, Xiaoyan Sun, Houqiang Li</P>
         

            <P class="ys2"><A href="https://sds.ustc.edu.cn/"
            target="_new">School of Data Science</A></P>
            <P class="ys2"><A href="http://leinao.ustc.edu.cn/"
            target="_new">National Engineering Laboratory for Brain-inspired Intelligence Technology and Application</A></P>
            <P  class="ys2"><A href="http://www.ustc.edu.cn/" 
            target="_new">University of Science and Technology of China</A></P>
            <P align="center" class="ys2">Tencent Data Platform</P>
            <P class="ys2">Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS-2021)</P></TD></TR>



        <TR>
          <TD>
            <P class="title1">Abstract:</P></TD></TR>
        <TR>
          <TD>
            <P class="text2"> Generalized Zero-Shot Learning (GZSL) aims to recognize new categories with auxiliary semantic information, e.g., category attributes.
	In this paper, we handle the critical issue of domain shift problem, i.e., confusion between seen and unseen categories, by progressively improving cross-domain transferability and category discriminability of visual representations.
	Our approach, named Dual Progressive Prototype Network (DPPN), constructs two types of prototypes that record prototypical visual patterns for attributes and categories, respectively.
	With attribute prototypes, DPPN alternately searches attribute-related local regions and updates corresponding attribute prototypes to progressively explore accurate attribute-region correspondence.
	This enables DPPN to produce visual representations with accurate attribute localization ability, which benefits the semantic-visual alignment and representation transferability.
	Besides, along with progressive attribute localization, DPPN further projects category prototypes into multiple spaces to progressively repel visual representations from different categories, which boosts category discriminability.
	Both attribute and category prototypes are collaboratively learned in a unified framework, which makes visual representations of DPPN transferable and distinctive.
	Experiments on four benchmarks prove that DPPN effectively alleviates the domain shift problem in GZSL.</P></TD></TR>
        <TR>
          <TD>&nbsp;</TD></TR>
        <TR>
          <TD>
            <DIV align="center"><IMG width="800" alt="AutomaticalResult" src="figures/moti.png"></DIV></TD></TR>
        <TR>
          <TD>
            <DIV align="center"><SPAN class="teaser">Figure 1:</SPAN> <SPAN class="text">The motivation of DPPN. (a) General GZSL methods directly align global image features with category attributes. 
              (b) A typical part-based method, i.e., APN [1], learns prototypes shared by all images for attribute localization. 
              (c) DPPN progressively adjusts prototypes according to different images and introduces category prototypes to enhance category discriminability.</SPAN></DIV></TD></TR>
        <TR>
          <TD>&nbsp;</TD></TR>

        <TR>
          <TD>
            <P class="title1">Results:</P></TD></TR>
        <TR>
          <TD>
            <DIV align="center"><IMG width="1000" alt="AutomaticalResult" src="figures/vis.png"></DIV></TD></TR>
        <TR>
          <TD>
            <DIV align="center"><SPAN class="teaser">Figure 2:</SPAN> <SPAN class="text">Visualization of attribute localization at different iterations. The localization gets more and more accurate as k increases from 0 to 2.</SPAN></DIV></TD></TR>
        <TR>
          <TD>&nbsp;</TD></TR>
        <TR>
          <TD>
            <DIV align="center"><IMG width="1000" alt="OptimizationResult" src="figures/k.png"></DIV></TD></TR>
        <TR>
          <TD>
            <SPAN class="teaser">Figure 3:</SPAN><SPAN class="text"> <SPAN class="text2">Effect of progressive updating with varying K on four datasets. 
            </SPAN></SPAN></DIV></TD></TR>
        <TR>
          <TD>&nbsp;</TD></TR>
        <TR>
          <TD>
            <DIV align="center"><IMG width="1000" alt="LayoutOptimizationResult" src="figures/comp.PNG"></DIV></TD></TR>
        <TR>
          <TD
            <SPAN class="teaser">Table 1:</SPAN><SPAN 
            class="text"> <SPAN class="text2">Effect of PCC and PAL on CUB and aPY datasets. GFLOPs is calculated with input size 
              448 × 448 on the CUB dataset.
            </SPAN></SPAN></DIV></TD></TR>
        <TR>
          <TD>&nbsp;</TD></TR>
        <TR>
        <TD>
            <DIV align="center"><IMG width="1000" alt="LayoutOptimizationResult" src="figures/final.PNG"></DIV></TD></TR>
        <TR>
          <TD>
            <SPAN class="teaser">Table 2:</SPAN><SPAN
            class="text"> <SPAN class="text2">Results of GZSL on four classification benchmarks. Generative methods (GEN) utilize extra synthetic unseen domain data for training.

            </SPAN></SPAN></DIV></TD></TR>
        <TR>
          <TD>&nbsp;</TD></TR>
        <TR>

       

        <TR>
          <TD><SPAN class="title1">Acknowledgements:</SPAN></TD></TR>
        <TR>
          <TD>
            <P class="text2"> This work was supported by National Natural Science Foundation of China (NSFC) under Grants 
              61632006 and 62076230.
			</P></TD></TR>
        <TR>
          <TD>&nbsp;</TD></TR>
       <TR>
          <TD><SPAN class="title1">Main References:</SPAN></TD></TR>
        <TR>
          <TD>
            <P class="text2"> [1] Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, Zeynep Akata. Attribute Prototype Network for Zero-Shot Learning. in NeurIPS 2020.
        </P></TD></TR>
        <TR>
        <TR>
          <TD>&nbsp;</TD></TR>
        <TR>
        
        <TR>
          <TD>&nbsp;</TD></TR>
        <TR>
          <TD>
            <UL>
              <LI class="download"><A href="https://arxiv.org/pdf/2111.02073.pdf">Paper
              </A></LI>
              <LI class="download"><A href="https://github.com/Roxanne-Wang/DPPN-GZSL">Code</A></LI></UL></TD></TR></TBODY></TABLE>
      <TABLE width="900" align="center">
        <TBODY>
        <TR>
          <TD>
            <HR>
          </TD></TR>
        <TR>
          <TD class="copy1">Copyright © 2018 GCL&nbsp;, 
      USTC</TD></TR></TBODY></TABLE>
      <P>&nbsp;</P></TD></TR></TBODY></TABLE></BODY></HTML>
